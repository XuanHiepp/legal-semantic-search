{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylcAINFtgns3"
      },
      "source": [
        "## 1. Install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yc8iF9vbgsHV"
      },
      "outputs": [],
      "source": [
        "!pip install underthesea pandas\n",
        "!pip install qdrant-client sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJw0mivBgxPt"
      },
      "source": [
        "## 2. Upload data files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65mQe9dOb0af"
      },
      "outputs": [],
      "source": [
        "!unzip \"TVPL AI.zip\" -d TVPL_AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBKiuWlPg0-A"
      },
      "source": [
        "## 3. Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuhbLwq-hFvS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "from underthesea import sent_tokenize\n",
        "import zipfile\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "data_dir = './data'\n",
        "output_dir = os.path.join(data_dir, 'processed')\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "data_dir = 'TVPL_AI/data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gw6R72C8hDxA"
      },
      "outputs": [],
      "source": [
        "def repl(match):\n",
        "    num = float(match.group(1))\n",
        "    return f\"{num * 1_000_000:,.0f} VNĐ\".replace(\",\", \".\")\n",
        "\n",
        "def normalize_number(text):\n",
        "    \"\"\"\n",
        "    Convert number formats 1.2m -> 1.200.000 VNĐ\n",
        "    \"\"\"\n",
        "    return re.sub(r'(\\d+\\.?\\d*)m\\b', repl, text)\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Remove special characters, normalize spaces.\n",
        "    \"\"\"\n",
        "    text = re.sub(r'[^\\w\\sÀ-ỹ.,:;\\n]', '', text)  # Keep the period, line break\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def extract_title(after_dieu):\n",
        "    \"\"\"\n",
        "    Excerpt title and end position.\n",
        "    Allow long titles, commas, line breaks.\n",
        "    \"\"\"\n",
        "    m = re.search(r'(.+?)(?=\\n\\d+\\.\\s|\\n\\d+\\.|\\n|$)', after_dieu)\n",
        "    if m:\n",
        "        title = m.group(1).strip()\n",
        "        end_pos = m.end()\n",
        "    else:\n",
        "        title = after_dieu.strip()\n",
        "        end_pos = len(after_dieu)\n",
        "\n",
        "    return title, end_pos\n",
        "\n",
        "def split_articles(raw_text):\n",
        "    \"\"\"\n",
        "    Split text into articles ONLY if Article X has a period.\n",
        "    \"\"\"\n",
        "    pattern = r'(\\[BOLD\\]Điều\\s+\\d+\\s*\\.)'\n",
        "    matches = list(re.finditer(pattern, raw_text, re.IGNORECASE))\n",
        "    articles = {}\n",
        "\n",
        "    for idx, match in enumerate(matches):\n",
        "        start = match.start()\n",
        "        end = matches[idx + 1].start() if idx + 1 < len(matches) else len(raw_text)\n",
        "        article_text = raw_text[start:end].strip()\n",
        "\n",
        "        # Get number only\n",
        "        m = re.search(r'Điều\\s+(\\d+)', match.group(1), re.IGNORECASE)\n",
        "        if not m:\n",
        "            continue\n",
        "        article_num = m.group(1)\n",
        "\n",
        "        after_dieu = article_text[len(match.group(1)):].strip()\n",
        "        title, title_end = extract_title(after_dieu)\n",
        "        content = after_dieu[title_end:].strip()\n",
        "\n",
        "        # If content is empty => skip\n",
        "        if not content:\n",
        "            print(f\"Skip Article {article_num} because text is blank.\")\n",
        "            continue\n",
        "\n",
        "        articles[f\"dieu_{article_num}\"] = {\n",
        "            \"title\": title.replace(\"[BOLD]\", \"\").strip(),\n",
        "            \"text\": content.replace(\"[BOLD]\", \"\").strip()\n",
        "        }\n",
        "\n",
        "    return articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "h6HuMhM5MFWV",
        "outputId": "25c4a86e-7786-4f20-8d1a-2598260c2dd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5 file .docx in TVPL_AI/data\n",
            "100_2015_QH13_296661.docx: Split 415 law\n",
            "Processed and saved: ./data/processed/100_2015_QH13_296661.json\n",
            "145_2020_ND-CP_459400.docx: Split 0 law\n",
            "Processed and saved: ./data/processed/145_2020_ND-CP_459400.json\n",
            "41_2024_QH15_557190.docx: Split 137 law\n",
            "Processed and saved: ./data/processed/41_2024_QH15_557190.json\n",
            "168_2024_ND-CP_619502.docx: Split 54 law\n",
            "Processed and saved: ./data/processed/168_2024_ND-CP_619502.json\n",
            "45_2019_QH14_333670.docx: Split 214 law\n",
            "Processed and saved: ./data/processed/45_2019_QH14_333670.json\n"
          ]
        }
      ],
      "source": [
        "def read_docx(docx_path):\n",
        "    \"\"\"\n",
        "    Read the DOCX content, distinguishing bold words.\n",
        "    \"\"\"\n",
        "    import zipfile\n",
        "    import xml.etree.ElementTree as ET\n",
        "\n",
        "    with zipfile.ZipFile(docx_path) as z:\n",
        "        xml_content = z.read('word/document.xml')\n",
        "\n",
        "    tree = ET.XML(xml_content)\n",
        "    ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
        "\n",
        "    paragraphs = []\n",
        "    for para in tree.findall('.//w:p', ns):\n",
        "        runs = []\n",
        "\n",
        "        for run in para.findall('.//w:r', ns):\n",
        "            # Check bold\n",
        "            rPr = run.find('w:rPr', ns)\n",
        "            is_bold = False\n",
        "            if rPr is not None and rPr.find('w:b', ns) is not None:\n",
        "                is_bold = True\n",
        "\n",
        "            text_elem = run.find('w:t', ns)\n",
        "            if text_elem is not None and text_elem.text:\n",
        "                if is_bold:\n",
        "                    runs.append(f\"[BOLD]{text_elem.text}\")\n",
        "                else:\n",
        "                    runs.append(text_elem.text)\n",
        "\n",
        "        if runs:\n",
        "            paragraphs.append(''.join(runs))\n",
        "\n",
        "    return '\\n'.join(paragraphs)\n",
        "\n",
        "def process_files():\n",
        "    all_files = [f for f in os.listdir(data_dir) if f.endswith('.docx')]\n",
        "    print(f\"Found {len(all_files)} file .docx in {data_dir}\")\n",
        "\n",
        "    for file_name in all_files:\n",
        "        file_path = os.path.join(data_dir, file_name)\n",
        "        raw_text = read_docx(file_path)\n",
        "\n",
        "        # Section of the law\n",
        "        result = split_articles(raw_text)\n",
        "        print(f\"{file_name}: Split {len(result)} law\")\n",
        "\n",
        "        # Save JSON to processed folder\n",
        "        json_file = file_name.replace('.docx', '.json')\n",
        "        json_path = os.path.join(output_dir, json_file)\n",
        "        with open(json_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(result, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"Processed and saved: {json_path}\")\n",
        "\n",
        "# Run all process\n",
        "process_files()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KymbmFKChPPb"
      },
      "source": [
        "## 4. Save new json files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PA5Xb0wggcve",
        "outputId": "e043c39d-850d-4991-bf41-c2188d98b5a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Number of Articles: 820\n",
            "Full metadata saved, ready to be encoded.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "data_dir = \"data/processed\"\n",
        "all_files = [f for f in os.listdir(data_dir) if f.endswith('.json')]\n",
        "\n",
        "texts = []\n",
        "metadata = []\n",
        "\n",
        "for file_name in all_files:\n",
        "    file_path = os.path.join(data_dir, file_name)\n",
        "    with open(file_path, encoding=\"utf-8\") as f:\n",
        "        laws = json.load(f)\n",
        "\n",
        "    for key, val in laws.items():\n",
        "        full_text = val[\"title\"] + \". \" + val[\"text\"]\n",
        "        texts.append(full_text)\n",
        "\n",
        "        # Add file name to ID to avoid duplication\n",
        "        unique_id = f\"{file_name.replace('.json', '')}_{key}\"\n",
        "\n",
        "        metadata.append({\n",
        "            \"id\": unique_id,\n",
        "            \"file\": file_name,\n",
        "            \"title\": val[\"title\"],\n",
        "            \"text\": val[\"text\"]\n",
        "        })\n",
        "\n",
        "print(f\"Total Number of Articles: {len(texts)}\")\n",
        "\n",
        "# Save metadata\n",
        "with open(\"laws_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Full metadata saved, ready to be encoded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWZoXLzRhVVE"
      },
      "source": [
        "## 5. Encode the articles and save them to Qdrant Cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "mL-AjcJXjadk",
        "outputId": "c6f4c8f7-8ff8-43d4-e1f4-e4336b54b4eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name VoVanPhuc/sup-SimCSE-VietNamese-phobert-base. Creating a new one with mean pooling.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upsert the Rule into Qdrant.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import PointStruct, Distance, VectorParams\n",
        "import json\n",
        "\n",
        "# 1. Cloud connectivity\n",
        "QDRANT_URL=\"https://b0214c11-fe06-459c-9404-70e50eeef994.europe-west3-0.gcp.cloud.qdrant.io:6333\"\n",
        "QDRANT_API_KEY=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.bIifyGGrvIdKotUPhmX-z-AXiFw90fGUpaNpN4js1f8\"\n",
        "qdrant_client = QdrantClient(\n",
        "    url=QDRANT_URL,\n",
        "    api_key=QDRANT_API_KEY,\n",
        ")\n",
        "\n",
        "# 2. Create collection if not exist\n",
        "if qdrant_client.collection_exists(\"laws_collection\"):\n",
        "    qdrant_client.delete_collection(\"laws_collection\")\n",
        "\n",
        "qdrant_client.create_collection(\n",
        "    collection_name=\"laws_collection\",\n",
        "    vectors_config=VectorParams(size=768, distance=Distance.COSINE)\n",
        ")\n",
        "\n",
        "# 3. Load JSON\n",
        "with open(\"laws_metadata.json\", encoding=\"utf-8\") as f:\n",
        "    laws = json.load(f)\n",
        "\n",
        "# 4. Encode the Law\n",
        "model = SentenceTransformer(\"VoVanPhuc/sup-SimCSE-VietNamese-phobert-base\", device='cuda')\n",
        "\n",
        "texts = [law[\"title\"] + \". \" + law[\"text\"] for law in laws]\n",
        "embeddings = model.encode(\n",
        "    texts,\n",
        "    batch_size=32,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True\n",
        ")\n",
        "\n",
        "# 5. Upsert into Qdrant\n",
        "points = []\n",
        "for idx, (vec, law) in enumerate(zip(embeddings, laws)):\n",
        "    points.append(PointStruct(\n",
        "        id=idx,\n",
        "        vector=vec.tolist(),\n",
        "        payload={\n",
        "            \"id\": law[\"id\"],\n",
        "            \"file\": law.get(\"file\", \"\"),\n",
        "            \"title\": law[\"title\"],\n",
        "            \"text\": law[\"text\"]\n",
        "        }\n",
        "    ))\n",
        "\n",
        "qdrant_client.upsert(\n",
        "    collection_name=\"laws_collection\",\n",
        "    points=points\n",
        ")\n",
        "\n",
        "print(\"Upsert the Rule into Qdrant.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcuE7actheAJ"
      },
      "source": [
        "## 6. Connect Qdrant cloud and look up the answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alXe2EZG4-ul",
        "outputId": "0f7b3ccc-aefc-4638-a776-3e359ac8e41a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name VoVanPhuc/sup-SimCSE-VietNamese-phobert-base. Creating a new one with mean pooling.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5 hits\n",
            "ID: 45_2019_QH14_333670_dieu_207\n",
            "Điều: Tiền lương và các quyền lợi hợp pháp khác của người lao động trong thời gian đình công\n",
            "Score: 0.6874\n",
            "---\n",
            "ID: 45_2019_QH14_333670_dieu_90\n",
            "Điều: Tiền lương\n",
            "Score: 0.6714\n",
            "---\n",
            "ID: 45_2019_QH14_333670_dieu_40\n",
            "Điều: Nghĩa vụ của người lao động khi đơn phương chấm dứt hợp đồng lao động trái pháp luật\n",
            "Score: 0.6681\n",
            "---\n",
            "ID: 45_2019_QH14_333670_dieu_99\n",
            "Điều: Tiền lương ngừng việc\n",
            "Score: 0.6635\n",
            "---\n",
            "ID: 41_2024_QH15_557190_dieu_31\n",
            "Điều: Căn cứ đóng bảo hiểm xã hội\n",
            "Score: 0.6607\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from qdrant_client import QdrantClient\n",
        "\n",
        "# Encode the question\n",
        "user_query = \"NLĐ bị sa thải có được trả lương hay không?\"\n",
        "model = SentenceTransformer(\"VoVanPhuc/sup-SimCSE-VietNamese-phobert-base\", device='cuda')\n",
        "query_vec = model.encode(user_query, normalize_embeddings=True)\n",
        "\n",
        "# Conect the Qdrant cloud\n",
        "qdrant_client = QdrantClient(\n",
        "    url=QDRANT_URL,\n",
        "    api_key=QDRANT_API_KEY,\n",
        ")\n",
        "results = qdrant_client.query_points(\n",
        "    collection_name=\"laws_collection\",\n",
        "    query=query_vec.tolist(),\n",
        "    limit=5,\n",
        "    with_payload=True\n",
        ").points\n",
        "\n",
        "print(f\"Found {len(results)} hits\")\n",
        "for hit in results:\n",
        "    print(f\"ID: {hit.payload['id']}\")\n",
        "    print(f\"Điều: {hit.payload['title']}\")\n",
        "    print(f\"Score: {hit.score:.4f}\")\n",
        "    print(\"---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6BA7nzgh2CO"
      },
      "source": [
        "## 7. Search with large number of queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0edp_XXFCk4",
        "outputId": "7f58b74a-eaa4-44f1-a933-e4d7db40e72f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved to file 'search_results.txt'.\n"
          ]
        }
      ],
      "source": [
        "# List of questions\n",
        "queries = [\n",
        "    \"NLĐ bị sa thải có được trả lương hay không?\",\n",
        "    \"Người sử dụng lao động được sa thải người lao động nữ đang mang thai không?\",\n",
        "    \"Quy định về điều chuyển nhân sự được quy định như thế nào?\",\n",
        "    \"Người lao động được thuê làm giám đốc doanh nghiệp Nhà nước được hưởng các chế độ về tiền lương, thưởng như thế nào?\",\n",
        "    \"Làm việc 8h một ngày thì được nghỉ giữa giờ ít nhất bao nhiêu phút?\",\n",
        "    \"Người sử dụng lao động đào tạo nghề nghiệp và phát triển kỹ năng nghề cho người lao động như thế nào?\",\n",
        "    \"Nguyên tắc cho thuê lại lao động là gì?\",\n",
        "    \"Thời hạn của thỏa ước lao động tập thể như thế nào?\",\n",
        "    \"Hợp đồng lao động được giao kết theo hình thức nào?\",\n",
        "    \"Nội dung về đào tạo lao động có bắt buộc phải ghi vào hợp đồng lao động?\"\n",
        "]\n",
        "\n",
        "with open(\"search_results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    cnt = 1\n",
        "    for user_query in queries:\n",
        "        # Encode the question\n",
        "        query_vec = model.encode(user_query, normalize_embeddings=True)\n",
        "\n",
        "        # Query Qdrant to get results\n",
        "        results = qdrant_client.query_points(\n",
        "            collection_name=\"laws_collection\",\n",
        "            query=query_vec.tolist(),\n",
        "            limit=5,\n",
        "            with_payload=True\n",
        "        ).points\n",
        "\n",
        "        # Write results to file\n",
        "        arti_cnt = 1\n",
        "        f.write(f\"Query {cnt}: {user_query}\\n\")\n",
        "        for hit in results:\n",
        "            f.write(f\"Article {arti_cnt}: {hit.payload['title']}\\n\")\n",
        "            f.write(\"---\\n\")\n",
        "            arti_cnt += 1\n",
        "        f.write(\"\\n\\n\")\n",
        "\n",
        "        cnt += 1\n",
        "\n",
        "print(\"Results saved to file 'search_results.txt'.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
