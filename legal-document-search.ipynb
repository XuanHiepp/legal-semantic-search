{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. Install required libraries","metadata":{"id":"ylcAINFtgns3"}},{"cell_type":"code","source":"!pip install underthesea\n!pip install qdrant-client sentence-transformers","metadata":{"id":"Yc8iF9vbgsHV","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Upload data files","metadata":{"id":"DJw0mivBgxPt"}},{"cell_type":"code","source":"!unzip \"TVPL_AI.zip\" -d .","metadata":{"id":"65mQe9dOb0af","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Preprocess the data","metadata":{"id":"iBKiuWlPg0-A"}},{"cell_type":"code","source":"import os\nimport re\nimport json\nfrom underthesea import sent_tokenize\nimport xml.etree.ElementTree as ET\n\ndata_dir = 'data'\noutput_dir = os.path.join(data_dir, 'processed')\nos.makedirs(output_dir, exist_ok=True)\ndata_dir = 'TVPL_AI/data'","metadata":{"id":"DuhbLwq-hFvS","trusted":true,"execution":{"iopub.status.busy":"2025-07-09T22:08:46.291640Z","iopub.execute_input":"2025-07-09T22:08:46.292459Z","iopub.status.idle":"2025-07-09T22:08:46.297311Z","shell.execute_reply.started":"2025-07-09T22:08:46.292429Z","shell.execute_reply":"2025-07-09T22:08:46.296541Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def repl(match):\n    num = float(match.group(1))\n    return f\"{num * 1_000_000:,.0f} VNĐ\".replace(\",\", \".\")\n\ndef normalize_number(text):\n    \"\"\"\n    Convert number formats 1.2m -> 1.200.000 VNĐ\n    \"\"\"\n    return re.sub(r'(\\d+\\.?\\d*)m\\b', repl, text)\n\ndef clean_text(text):\n    \"\"\"\n    Remove special characters, normalize spaces.\n    \"\"\"\n    text = re.sub(r'[^\\w\\sÀ-ỹ.,:;\\n]', '', text)  # Keep the period, line break\n    text = re.sub(r'\\s+', ' ', text)\n    return text.strip()\n\ndef extract_title(after_dieu):\n    \"\"\"\n    Excerpt title and end position.\n    Allow long titles, commas, line breaks.\n    \"\"\"\n    m = re.search(r'(.+?)(?=\\n\\d+\\.\\s|\\n\\d+\\.|\\n|$)', after_dieu)\n    if m:\n        title = m.group(1).strip()\n        end_pos = m.end()\n    else:\n        title = after_dieu.strip()\n        end_pos = len(after_dieu)\n\n    return title, end_pos\n\ndef split_articles(raw_text):\n    \"\"\"\n    Split text into articles ONLY if Article X has a period.\n    \"\"\"\n    pattern = r'(\\[BOLD\\]Điều\\s+\\d+\\s*\\.)'\n    matches = list(re.finditer(pattern, raw_text, re.IGNORECASE))\n    articles = {}\n\n    for idx, match in enumerate(matches):\n        start = match.start()\n        end = matches[idx + 1].start() if idx + 1 < len(matches) else len(raw_text)\n        article_text = raw_text[start:end].strip()\n\n        # Get number only\n        m = re.search(r'Điều\\s+(\\d+)', match.group(1), re.IGNORECASE)\n        if not m:\n            continue\n        article_num = m.group(1)\n\n        after_dieu = article_text[len(match.group(1)):].strip()\n        title, title_end = extract_title(after_dieu)\n        content = after_dieu[title_end:].strip()\n\n        # If content is empty => skip\n        if not content:\n            print(f\"Skip Article {article_num} because text is blank.\")\n            continue\n\n        articles[f\"dieu_{article_num}\"] = {\n            \"title\": title.replace(\"[BOLD]\", \"\").strip(),\n            \"text\": content.replace(\"[BOLD]\", \"\").strip()\n        }\n\n    return articles","metadata":{"id":"Gw6R72C8hDxA","trusted":true,"execution":{"iopub.status.busy":"2025-07-09T22:08:47.224283Z","iopub.execute_input":"2025-07-09T22:08:47.224676Z","iopub.status.idle":"2025-07-09T22:08:47.234230Z","shell.execute_reply.started":"2025-07-09T22:08:47.224650Z","shell.execute_reply":"2025-07-09T22:08:47.233309Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def read_docx(docx_path):\n    \"\"\"\n    Read the DOCX content, distinguishing bold words.\n    \"\"\"\n    import zipfile\n    import xml.etree.ElementTree as ET\n\n    with zipfile.ZipFile(docx_path) as z:\n        xml_content = z.read('word/document.xml')\n\n    tree = ET.XML(xml_content)\n    ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n\n    paragraphs = []\n    for para in tree.findall('.//w:p', ns):\n        runs = []\n\n        for run in para.findall('.//w:r', ns):\n            # Check bold\n            rPr = run.find('w:rPr', ns)\n            is_bold = False\n            if rPr is not None and rPr.find('w:b', ns) is not None:\n                is_bold = True\n\n            text_elem = run.find('w:t', ns)\n            if text_elem is not None and text_elem.text:\n                if is_bold:\n                    runs.append(f\"[BOLD]{text_elem.text}\")\n                else:\n                    runs.append(text_elem.text)\n\n        if runs:\n            paragraphs.append(''.join(runs))\n\n    return '\\n'.join(paragraphs)\n\ndef process_files():\n    all_files = [f for f in os.listdir(data_dir) if f.endswith('.docx')]\n    print(f\"Found {len(all_files)} file .docx in {data_dir}\")\n\n    for file_name in all_files:\n        file_path = os.path.join(data_dir, file_name)\n        raw_text = read_docx(file_path)\n\n        # Section of the law\n        result = split_articles(raw_text)\n        print(f\"{file_name}: Split {len(result)} law\")\n\n        # Save JSON to processed folder\n        json_file = file_name.replace('.docx', '.json')\n        json_path = os.path.join(output_dir, json_file)\n        with open(json_path, 'w', encoding='utf-8') as f:\n            json.dump(result, f, ensure_ascii=False, indent=2)\n\n        print(f\"Processed and saved: {json_path}\")\n\n# Run all process\nprocess_files()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"h6HuMhM5MFWV","outputId":"25c4a86e-7786-4f20-8d1a-2598260c2dd3","trusted":true,"execution":{"iopub.status.busy":"2025-07-09T22:08:48.911474Z","iopub.execute_input":"2025-07-09T22:08:48.911775Z","iopub.status.idle":"2025-07-09T22:08:49.260275Z","shell.execute_reply.started":"2025-07-09T22:08:48.911756Z","shell.execute_reply":"2025-07-09T22:08:49.259621Z"}},"outputs":[{"name":"stdout","text":"Found 5 file .docx in TVPL_AI/data\n100_2015_QH13_296661.docx: Split 415 law\nProcessed and saved: data/processed/100_2015_QH13_296661.json\n45_2019_QH14_333670.docx: Split 214 law\nProcessed and saved: data/processed/45_2019_QH14_333670.json\n168_2024_ND-CP_619502.docx: Split 54 law\nProcessed and saved: data/processed/168_2024_ND-CP_619502.json\n41_2024_QH15_557190.docx: Split 137 law\nProcessed and saved: data/processed/41_2024_QH15_557190.json\n145_2020_ND-CP_459400.docx: Split 0 law\nProcessed and saved: data/processed/145_2020_ND-CP_459400.json\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"## 4. Save new json files","metadata":{"id":"KymbmFKChPPb"}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport json\n\ndata_dir = \"data/processed\"\nall_files = [f for f in os.listdir(data_dir) if f.endswith('.json')]\n\ntexts = []\nmetadata = []\n\nfor file_name in all_files:\n    file_path = os.path.join(data_dir, file_name)\n    with open(file_path, encoding=\"utf-8\") as f:\n        laws = json.load(f)\n\n    for key, val in laws.items():\n        full_text = val[\"title\"] + \". \" + val[\"text\"]\n        texts.append(full_text)\n\n        # Add file name to ID to avoid duplication\n        unique_id = f\"{file_name.replace('.json', '')}_{key}\"\n\n        metadata.append({\n            \"id\": unique_id,\n            \"file\": file_name,\n            \"title\": val[\"title\"],\n            \"text\": val[\"text\"]\n        })\n\nprint(f\"Total Number of Articles: {len(texts)}\")\n\n# Save metadata\nwith open(\"laws_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(metadata, f, ensure_ascii=False, indent=2)\n\nprint(\"Full metadata saved, ready to be encoded.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"PA5Xb0wggcve","outputId":"e043c39d-850d-4991-bf41-c2188d98b5a4","trusted":true,"execution":{"iopub.status.busy":"2025-07-09T22:08:51.368606Z","iopub.execute_input":"2025-07-09T22:08:51.369221Z","iopub.status.idle":"2025-07-09T22:08:51.416438Z","shell.execute_reply.started":"2025-07-09T22:08:51.369194Z","shell.execute_reply":"2025-07-09T22:08:51.415802Z"}},"outputs":[{"name":"stdout","text":"Total Number of Articles: 820\nFull metadata saved, ready to be encoded.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"## 5. Encode the articles and save them to Qdrant Cloud","metadata":{"id":"pWZoXLzRhVVE"}},{"cell_type":"code","source":"import os\nos.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\nfrom sentence_transformers import SentenceTransformer\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import PointStruct, Distance, VectorParams\nimport json\n\n# 1. Cloud connectivity\nQDRANT_URL=\"https://f8972297-f3c1-4741-ac07-79b7dc248785.europe-west3-0.gcp.cloud.qdrant.io:6333\"\nQDRANT_API_KEY=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.M4sXz1nwgx_ZsJAn_d2AJDzEeuWo_0hClPX2fx2avAw\"\nqdrant_client = QdrantClient(\n    url=QDRANT_URL,\n    api_key=QDRANT_API_KEY,\n)\n\n# 2. Create collection if not exist\nif qdrant_client.collection_exists(\"laws_collection\"):\n    qdrant_client.delete_collection(\"laws_collection\")\n\nqdrant_client.create_collection(\n    collection_name=\"laws_collection\",\n    vectors_config=VectorParams(size=768, distance=Distance.COSINE)\n)\n\n# 3. Load JSON\nwith open(\"laws_metadata.json\", encoding=\"utf-8\") as f:\n    laws = json.load(f)\n\n# 4. Encode the Law\nmodel = SentenceTransformer(\"VoVanPhuc/sup-SimCSE-VietNamese-phobert-base\", device='cuda')\n\ntexts = [law[\"title\"] + \". \" + law[\"text\"] for law in laws]\nembeddings = model.encode(\n    texts,\n    batch_size=32,\n    convert_to_numpy=True,\n    normalize_embeddings=True\n)\n\n# 5. Upsert into Qdrant\npoints = []\nfor idx, (vec, law) in enumerate(zip(embeddings, laws)):\n    points.append(PointStruct(\n        id=idx,\n        vector=vec.tolist(),\n        payload={\n            \"id\": law[\"id\"],\n            \"file\": law.get(\"file\", \"\"),\n            \"title\": law[\"title\"],\n            \"text\": law[\"text\"]\n        }\n    ))\n\nqdrant_client.upsert(\n    collection_name=\"laws_collection\",\n    points=points\n)\n\nprint(\"Upsert the Rule into Qdrant.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"mL-AjcJXjadk","outputId":"c6f4c8f7-8ff8-43d4-e1f4-e4336b54b4eb","trusted":true,"execution":{"iopub.status.busy":"2025-07-09T22:08:54.394071Z","iopub.execute_input":"2025-07-09T22:08:54.394353Z","iopub.status.idle":"2025-07-09T22:09:06.579577Z","shell.execute_reply.started":"2025-07-09T22:08:54.394333Z","shell.execute_reply":"2025-07-09T22:09:06.578858Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4c5185de6c340cb962bba46a8850d13"}},"metadata":{}},{"name":"stdout","text":"Upsert the Rule into Qdrant.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"## 6. Connect Qdrant cloud and look up the answers","metadata":{"id":"FcuE7actheAJ"}},{"cell_type":"code","source":"import os\nos.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\nfrom sentence_transformers import SentenceTransformer\nfrom qdrant_client import QdrantClient\n\n# Encode the question\nuser_query = \"NLĐ bị sa thải có được trả lương hay không?\"\nmodel = SentenceTransformer(\"VoVanPhuc/sup-SimCSE-VietNamese-phobert-base\", device='cuda')\nquery_vec = model.encode(user_query, normalize_embeddings=True)\n\n# Conect the Qdrant cloud\nqdrant_client = QdrantClient(\n    url=QDRANT_URL,\n    api_key=QDRANT_API_KEY,\n)\nresults = qdrant_client.query_points(\n    collection_name=\"laws_collection\",\n    query=query_vec.tolist(),\n    limit=20,\n    with_payload=True\n).points\n\nseen_titles = set()\nunique_hits = []\n\nfor hit in results:\n    title = hit.payload['title'].strip()\n\n    if title in seen_titles:\n        continue\n\n    seen_titles.add(title)\n    unique_hits.append(hit)\n\n    if len(unique_hits) >= 5:\n        break\n\nfor idx, hit in enumerate(unique_hits, 1):\n    print(f\"Top {idx}:\")\n    print(f\"ID: {hit.payload['id']}\")\n    print(f\"Title: {hit.payload['title']}\")\n    print(f\"Score: {hit.score:.4f}\")\n    print(\"---\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"alXe2EZG4-ul","outputId":"0f7b3ccc-aefc-4638-a776-3e359ac8e41a","trusted":true,"execution":{"iopub.status.busy":"2025-07-09T22:09:08.358922Z","iopub.execute_input":"2025-07-09T22:09:08.359202Z","iopub.status.idle":"2025-07-09T22:09:10.658440Z","shell.execute_reply.started":"2025-07-09T22:09:08.359185Z","shell.execute_reply":"2025-07-09T22:09:10.657673Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5eab8e737af64837a2e8f33c1deba788"}},"metadata":{}},{"name":"stdout","text":"Top 1:\nID: 45_2019_QH14_333670_dieu_207\nTitle: Tiền lương và các quyền lợi hợp pháp khác của người lao động trong thời gian đình công\nScore: 0.6874\n---\nTop 2:\nID: 45_2019_QH14_333670_dieu_90\nTitle: Tiền lương\nScore: 0.6714\n---\nTop 3:\nID: 45_2019_QH14_333670_dieu_40\nTitle: Nghĩa vụ của người lao động khi đơn phương chấm dứt hợp đồng lao động trái pháp luật\nScore: 0.6681\n---\nTop 4:\nID: 45_2019_QH14_333670_dieu_99\nTitle: Tiền lương ngừng việc\nScore: 0.6635\n---\nTop 5:\nID: 41_2024_QH15_557190_dieu_31\nTitle: Căn cứ đóng bảo hiểm xã hội\nScore: 0.6607\n---\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"## 7. Search with large number of queries","metadata":{"id":"x6BA7nzgh2CO"}},{"cell_type":"code","source":"# List of questions\nqueries = [\n    \"NLĐ bị sa thải có được trả lương hay không?\",\n    \"Người sử dụng lao động được sa thải người lao động nữ đang mang thai không?\",\n    \"Quy định về điều chuyển nhân sự được quy định như thế nào?\",\n    \"Người lao động được thuê làm giám đốc doanh nghiệp Nhà nước được hưởng các chế độ về tiền lương, thưởng như thế nào?\",\n    \"Làm việc 8h một ngày thì được nghỉ giữa giờ ít nhất bao nhiêu phút?\",\n    \"Người sử dụng lao động đào tạo nghề nghiệp và phát triển kỹ năng nghề cho người lao động như thế nào?\",\n    \"Nguyên tắc cho thuê lại lao động là gì?\",\n    \"Thời hạn của thỏa ước lao động tập thể như thế nào?\",\n    \"Hợp đồng lao động được giao kết theo hình thức nào?\",\n    \"Nội dung về đào tạo lao động có bắt buộc phải ghi vào hợp đồng lao động?\"\n]\n\nwith open(\"search_results.txt\", \"w\", encoding=\"utf-8\") as f:\n    cnt = 1\n    for user_query in queries:\n        # Encode the question\n        query_vec = model.encode(user_query, normalize_embeddings=True)\n\n        # Query Qdrant to get results\n        hits = qdrant_client.query_points(\n            collection_name=\"laws_collection\",\n            query=query_vec.tolist(),\n            limit=20,\n            with_payload=True\n        ).points\n\n        seen_titles = set()\n        arti_cnt = 1\n\n        f.write(f\"Query {cnt}: {user_query}\\n\")\n\n        for hit in hits:\n            title = hit.payload['title'].strip()\n\n            if title in seen_titles:\n                continue\n\n            seen_titles.add(title)\n\n            f.write(f\"Article {arti_cnt}: {title}\\n\")\n            f.write(\"---\\n\")\n            arti_cnt += 1\n\n            # Stop if there are 5 unique results\n            if arti_cnt > 5:\n                break\n\n        f.write(\"\\n\\n\")\n        cnt += 1\n\nprint(\"Results saved to file 'search_results.txt'.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A0edp_XXFCk4","outputId":"7f58b74a-eaa4-44f1-a933-e4d7db40e72f","trusted":true,"execution":{"iopub.status.busy":"2025-07-09T22:09:14.906135Z","iopub.execute_input":"2025-07-09T22:09:14.906666Z","iopub.status.idle":"2025-07-09T22:09:16.836547Z","shell.execute_reply.started":"2025-07-09T22:09:14.906640Z","shell.execute_reply":"2025-07-09T22:09:16.835937Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed79c6165e424d09958ef9d8a08e8f11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"792443e6967d4b5c9327be25e6921a1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec7e2d079f4c4c628db29a362530c976"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41621b21c0b44fa4988dfb541a9dab8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ca9b43d0d7d4dbdb86e6936995215bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b009ebb7a10e495c82f67335f56fee15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c566f80398064547a64d020a3bf3343c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aee3acd06794568b0d6245aa4dfaf37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b3eac54bfba4597b9af383f9dd6f6ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5576a3815b3f45edbcd16eb104e48ff0"}},"metadata":{}},{"name":"stdout","text":"Results saved to file 'search_results.txt'.\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}