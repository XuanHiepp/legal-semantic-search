{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylcAINFtgns3"
   },
   "source": [
    "## 1. Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yc8iF9vbgsHV",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install underthesea\n",
    "!pip install qdrant-client sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJw0mivBgxPt"
   },
   "source": [
    "## 2. Upload data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65mQe9dOb0af",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!unzip \"TVPL_AI.zip\" -d ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBKiuWlPg0-A"
   },
   "source": [
    "## 3. Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T22:08:46.292459Z",
     "iopub.status.busy": "2025-07-09T22:08:46.291640Z",
     "iopub.status.idle": "2025-07-09T22:08:46.297311Z",
     "shell.execute_reply": "2025-07-09T22:08:46.296541Z",
     "shell.execute_reply.started": "2025-07-09T22:08:46.292429Z"
    },
    "id": "DuhbLwq-hFvS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from underthesea import sent_tokenize\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "data_dir = 'data'\n",
    "output_dir = os.path.join(data_dir, 'processed')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "data_dir = 'TVPL_AI/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T22:08:47.224676Z",
     "iopub.status.busy": "2025-07-09T22:08:47.224283Z",
     "iopub.status.idle": "2025-07-09T22:08:47.234230Z",
     "shell.execute_reply": "2025-07-09T22:08:47.233309Z",
     "shell.execute_reply.started": "2025-07-09T22:08:47.224650Z"
    },
    "id": "Gw6R72C8hDxA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def repl(match):\n",
    "    num = float(match.group(1))\n",
    "    return f\"{num * 1_000_000:,.0f} VNĐ\".replace(\",\", \".\")\n",
    "\n",
    "def normalize_number(text):\n",
    "    \"\"\"\n",
    "    Convert number formats 1.2m -> 1.200.000 VNĐ\n",
    "    \"\"\"\n",
    "    return re.sub(r'(\\d+\\.?\\d*)m\\b', repl, text)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Remove special characters, normalize spaces.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'[^\\w\\sÀ-ỹ.,:;\\n]', '', text)  # Keep the period, line break\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_title(after_dieu):\n",
    "    \"\"\"\n",
    "    Excerpt title and end position.\n",
    "    Allow long titles, commas, line breaks.\n",
    "    \"\"\"\n",
    "    m = re.search(r'(.+?)(?=\\n\\d+\\.\\s|\\n\\d+\\.|\\n|$)', after_dieu)\n",
    "    if m:\n",
    "        title = m.group(1).strip()\n",
    "        end_pos = m.end()\n",
    "    else:\n",
    "        title = after_dieu.strip()\n",
    "        end_pos = len(after_dieu)\n",
    "\n",
    "    return title, end_pos\n",
    "\n",
    "def split_articles(raw_text):\n",
    "    \"\"\"\n",
    "    Split text into articles ONLY if Article X has a period.\n",
    "    \"\"\"\n",
    "    pattern = r'(\\[BOLD\\]Điều\\s+\\d+\\s*\\.)'\n",
    "    matches = list(re.finditer(pattern, raw_text, re.IGNORECASE))\n",
    "    articles = {}\n",
    "\n",
    "    for idx, match in enumerate(matches):\n",
    "        start = match.start()\n",
    "        end = matches[idx + 1].start() if idx + 1 < len(matches) else len(raw_text)\n",
    "        article_text = raw_text[start:end].strip()\n",
    "\n",
    "        # Get number only\n",
    "        m = re.search(r'Điều\\s+(\\d+)', match.group(1), re.IGNORECASE)\n",
    "        if not m:\n",
    "            continue\n",
    "        article_num = m.group(1)\n",
    "\n",
    "        after_dieu = article_text[len(match.group(1)):].strip()\n",
    "        title, title_end = extract_title(after_dieu)\n",
    "        content = after_dieu[title_end:].strip()\n",
    "\n",
    "        # If content is empty => skip\n",
    "        if not content:\n",
    "            print(f\"Skip Article {article_num} because text is blank.\")\n",
    "            continue\n",
    "\n",
    "        articles[f\"dieu_{article_num}\"] = {\n",
    "            \"title\": title.replace(\"[BOLD]\", \"\").strip(),\n",
    "            \"text\": content.replace(\"[BOLD]\", \"\").strip()\n",
    "        }\n",
    "\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "execution": {
     "iopub.execute_input": "2025-07-09T22:08:48.911775Z",
     "iopub.status.busy": "2025-07-09T22:08:48.911474Z",
     "iopub.status.idle": "2025-07-09T22:08:49.260275Z",
     "shell.execute_reply": "2025-07-09T22:08:49.259621Z",
     "shell.execute_reply.started": "2025-07-09T22:08:48.911756Z"
    },
    "id": "h6HuMhM5MFWV",
    "outputId": "25c4a86e-7786-4f20-8d1a-2598260c2dd3",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 file .docx in TVPL_AI/data\n",
      "100_2015_QH13_296661.docx: Split 415 law\n",
      "Processed and saved: data/processed/100_2015_QH13_296661.json\n",
      "45_2019_QH14_333670.docx: Split 214 law\n",
      "Processed and saved: data/processed/45_2019_QH14_333670.json\n",
      "168_2024_ND-CP_619502.docx: Split 54 law\n",
      "Processed and saved: data/processed/168_2024_ND-CP_619502.json\n",
      "41_2024_QH15_557190.docx: Split 137 law\n",
      "Processed and saved: data/processed/41_2024_QH15_557190.json\n",
      "145_2020_ND-CP_459400.docx: Split 0 law\n",
      "Processed and saved: data/processed/145_2020_ND-CP_459400.json\n"
     ]
    }
   ],
   "source": [
    "def read_docx(docx_path):\n",
    "    \"\"\"\n",
    "    Read the DOCX content, distinguishing bold words.\n",
    "    \"\"\"\n",
    "    import zipfile\n",
    "    import xml.etree.ElementTree as ET\n",
    "\n",
    "    with zipfile.ZipFile(docx_path) as z:\n",
    "        xml_content = z.read('word/document.xml')\n",
    "\n",
    "    tree = ET.XML(xml_content)\n",
    "    ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
    "\n",
    "    paragraphs = []\n",
    "    for para in tree.findall('.//w:p', ns):\n",
    "        runs = []\n",
    "\n",
    "        for run in para.findall('.//w:r', ns):\n",
    "            # Check bold\n",
    "            rPr = run.find('w:rPr', ns)\n",
    "            is_bold = False\n",
    "            if rPr is not None and rPr.find('w:b', ns) is not None:\n",
    "                is_bold = True\n",
    "\n",
    "            text_elem = run.find('w:t', ns)\n",
    "            if text_elem is not None and text_elem.text:\n",
    "                if is_bold:\n",
    "                    runs.append(f\"[BOLD]{text_elem.text}\")\n",
    "                else:\n",
    "                    runs.append(text_elem.text)\n",
    "\n",
    "        if runs:\n",
    "            paragraphs.append(''.join(runs))\n",
    "\n",
    "    return '\\n'.join(paragraphs)\n",
    "\n",
    "def process_files():\n",
    "    all_files = [f for f in os.listdir(data_dir) if f.endswith('.docx')]\n",
    "    print(f\"Found {len(all_files)} file .docx in {data_dir}\")\n",
    "\n",
    "    for file_name in all_files:\n",
    "        file_path = os.path.join(data_dir, file_name)\n",
    "        raw_text = read_docx(file_path)\n",
    "\n",
    "        # Section of the law\n",
    "        result = split_articles(raw_text)\n",
    "        print(f\"{file_name}: Split {len(result)} law\")\n",
    "\n",
    "        # Save JSON to processed folder\n",
    "        json_file = file_name.replace('.docx', '.json')\n",
    "        json_path = os.path.join(output_dir, json_file)\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"Processed and saved: {json_path}\")\n",
    "\n",
    "# Run all process\n",
    "process_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KymbmFKChPPb"
   },
   "source": [
    "## 4. Save new json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "execution": {
     "iopub.execute_input": "2025-07-09T22:08:51.369221Z",
     "iopub.status.busy": "2025-07-09T22:08:51.368606Z",
     "iopub.status.idle": "2025-07-09T22:08:51.416438Z",
     "shell.execute_reply": "2025-07-09T22:08:51.415802Z",
     "shell.execute_reply.started": "2025-07-09T22:08:51.369194Z"
    },
    "id": "PA5Xb0wggcve",
    "outputId": "e043c39d-850d-4991-bf41-c2188d98b5a4",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Articles: 820\n",
      "Full metadata saved, ready to be encoded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "data_dir = \"data/processed\"\n",
    "all_files = [f for f in os.listdir(data_dir) if f.endswith('.json')]\n",
    "\n",
    "texts = []\n",
    "metadata = []\n",
    "\n",
    "for file_name in all_files:\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        laws = json.load(f)\n",
    "\n",
    "    for key, val in laws.items():\n",
    "        full_text = val[\"title\"] + \". \" + val[\"text\"]\n",
    "        texts.append(full_text)\n",
    "\n",
    "        # Add file name to ID to avoid duplication\n",
    "        unique_id = f\"{file_name.replace('.json', '')}_{key}\"\n",
    "\n",
    "        metadata.append({\n",
    "            \"id\": unique_id,\n",
    "            \"file\": file_name,\n",
    "            \"title\": val[\"title\"],\n",
    "            \"text\": val[\"text\"]\n",
    "        })\n",
    "\n",
    "print(f\"Total Number of Articles: {len(texts)}\")\n",
    "\n",
    "# Save metadata\n",
    "with open(\"laws_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Full metadata saved, ready to be encoded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWZoXLzRhVVE"
   },
   "source": [
    "## 5. Encode the articles and save them to Qdrant Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "execution": {
     "iopub.execute_input": "2025-07-09T22:08:54.394353Z",
     "iopub.status.busy": "2025-07-09T22:08:54.394071Z",
     "iopub.status.idle": "2025-07-09T22:09:06.579577Z",
     "shell.execute_reply": "2025-07-09T22:09:06.578858Z",
     "shell.execute_reply.started": "2025-07-09T22:08:54.394333Z"
    },
    "id": "mL-AjcJXjadk",
    "outputId": "c6f4c8f7-8ff8-43d4-e1f4-e4336b54b4eb",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c5185de6c340cb962bba46a8850d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upsert the Rule into Qdrant.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, Distance, VectorParams\n",
    "import json\n",
    "\n",
    "# 1. Cloud connectivity\n",
    "QDRANT_URL=\"https://f8972297-f3c1-4741-ac07-79b7dc248785.europe-west3-0.gcp.cloud.qdrant.io:6333\"\n",
    "QDRANT_API_KEY=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.M4sXz1nwgx_ZsJAn_d2AJDzEeuWo_0hClPX2fx2avAw\"\n",
    "qdrant_client = QdrantClient(\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    ")\n",
    "\n",
    "# 2. Create collection if not exist\n",
    "if qdrant_client.collection_exists(\"laws_collection\"):\n",
    "    qdrant_client.delete_collection(\"laws_collection\")\n",
    "\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=\"laws_collection\",\n",
    "    vectors_config=VectorParams(size=768, distance=Distance.COSINE)\n",
    ")\n",
    "\n",
    "# 3. Load JSON\n",
    "with open(\"laws_metadata.json\", encoding=\"utf-8\") as f:\n",
    "    laws = json.load(f)\n",
    "\n",
    "# 4. Encode the Law\n",
    "model = SentenceTransformer(\"VoVanPhuc/sup-SimCSE-VietNamese-phobert-base\", device='cuda')\n",
    "\n",
    "texts = [law[\"title\"] + \". \" + law[\"text\"] for law in laws]\n",
    "embeddings = model.encode(\n",
    "    texts,\n",
    "    batch_size=32,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "# 5. Upsert into Qdrant\n",
    "points = []\n",
    "for idx, (vec, law) in enumerate(zip(embeddings, laws)):\n",
    "    points.append(PointStruct(\n",
    "        id=idx,\n",
    "        vector=vec.tolist(),\n",
    "        payload={\n",
    "            \"id\": law[\"id\"],\n",
    "            \"file\": law.get(\"file\", \"\"),\n",
    "            \"title\": law[\"title\"],\n",
    "            \"text\": law[\"text\"]\n",
    "        }\n",
    "    ))\n",
    "\n",
    "qdrant_client.upsert(\n",
    "    collection_name=\"laws_collection\",\n",
    "    points=points\n",
    ")\n",
    "\n",
    "print(\"Upsert the Rule into Qdrant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcuE7actheAJ"
   },
   "source": [
    "## 6. Connect Qdrant cloud and look up the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-09T22:09:08.359202Z",
     "iopub.status.busy": "2025-07-09T22:09:08.358922Z",
     "iopub.status.idle": "2025-07-09T22:09:10.658440Z",
     "shell.execute_reply": "2025-07-09T22:09:10.657673Z",
     "shell.execute_reply.started": "2025-07-09T22:09:08.359185Z"
    },
    "id": "alXe2EZG4-ul",
    "outputId": "0f7b3ccc-aefc-4638-a776-3e359ac8e41a",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eab8e737af64837a2e8f33c1deba788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1:\n",
      "ID: 45_2019_QH14_333670_dieu_207\n",
      "Title: Tiền lương và các quyền lợi hợp pháp khác của người lao động trong thời gian đình công\n",
      "Score: 0.6874\n",
      "---\n",
      "Top 2:\n",
      "ID: 45_2019_QH14_333670_dieu_90\n",
      "Title: Tiền lương\n",
      "Score: 0.6714\n",
      "---\n",
      "Top 3:\n",
      "ID: 45_2019_QH14_333670_dieu_40\n",
      "Title: Nghĩa vụ của người lao động khi đơn phương chấm dứt hợp đồng lao động trái pháp luật\n",
      "Score: 0.6681\n",
      "---\n",
      "Top 4:\n",
      "ID: 45_2019_QH14_333670_dieu_99\n",
      "Title: Tiền lương ngừng việc\n",
      "Score: 0.6635\n",
      "---\n",
      "Top 5:\n",
      "ID: 41_2024_QH15_557190_dieu_31\n",
      "Title: Căn cứ đóng bảo hiểm xã hội\n",
      "Score: 0.6607\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# Encode the question\n",
    "user_query = \"NLĐ bị sa thải có được trả lương hay không?\"\n",
    "model = SentenceTransformer(\"VoVanPhuc/sup-SimCSE-VietNamese-phobert-base\", device='cuda')\n",
    "query_vec = model.encode(user_query, normalize_embeddings=True)\n",
    "\n",
    "# Conect the Qdrant cloud\n",
    "qdrant_client = QdrantClient(\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    ")\n",
    "results = qdrant_client.query_points(\n",
    "    collection_name=\"laws_collection\",\n",
    "    query=query_vec.tolist(),\n",
    "    limit=20,\n",
    "    with_payload=True\n",
    ").points\n",
    "\n",
    "seen_titles = set()\n",
    "unique_hits = []\n",
    "\n",
    "for hit in results:\n",
    "    title = hit.payload['title'].strip()\n",
    "\n",
    "    if title in seen_titles:\n",
    "        continue\n",
    "\n",
    "    seen_titles.add(title)\n",
    "    unique_hits.append(hit)\n",
    "\n",
    "    if len(unique_hits) >= 5:\n",
    "        break\n",
    "\n",
    "for idx, hit in enumerate(unique_hits, 1):\n",
    "    print(f\"Top {idx}:\")\n",
    "    print(f\"ID: {hit.payload['id']}\")\n",
    "    print(f\"Title: {hit.payload['title']}\")\n",
    "    print(f\"Score: {hit.score:.4f}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6BA7nzgh2CO"
   },
   "source": [
    "## 7. Search with large number of queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-09T22:09:14.906666Z",
     "iopub.status.busy": "2025-07-09T22:09:14.906135Z",
     "iopub.status.idle": "2025-07-09T22:09:16.836547Z",
     "shell.execute_reply": "2025-07-09T22:09:16.835937Z",
     "shell.execute_reply.started": "2025-07-09T22:09:14.906640Z"
    },
    "id": "A0edp_XXFCk4",
    "outputId": "7f58b74a-eaa4-44f1-a933-e4d7db40e72f",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed79c6165e424d09958ef9d8a08e8f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792443e6967d4b5c9327be25e6921a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7e2d079f4c4c628db29a362530c976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41621b21c0b44fa4988dfb541a9dab8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca9b43d0d7d4dbdb86e6936995215bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b009ebb7a10e495c82f67335f56fee15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c566f80398064547a64d020a3bf3343c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aee3acd06794568b0d6245aa4dfaf37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3eac54bfba4597b9af383f9dd6f6ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5576a3815b3f45edbcd16eb104e48ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to file 'search_results.txt'.\n"
     ]
    }
   ],
   "source": [
    "# List of questions\n",
    "queries = [\n",
    "    \"NLĐ bị sa thải có được trả lương hay không?\",\n",
    "    \"Người sử dụng lao động được sa thải người lao động nữ đang mang thai không?\",\n",
    "    \"Quy định về điều chuyển nhân sự được quy định như thế nào?\",\n",
    "    \"Người lao động được thuê làm giám đốc doanh nghiệp Nhà nước được hưởng các chế độ về tiền lương, thưởng như thế nào?\",\n",
    "    \"Làm việc 8h một ngày thì được nghỉ giữa giờ ít nhất bao nhiêu phút?\",\n",
    "    \"Người sử dụng lao động đào tạo nghề nghiệp và phát triển kỹ năng nghề cho người lao động như thế nào?\",\n",
    "    \"Nguyên tắc cho thuê lại lao động là gì?\",\n",
    "    \"Thời hạn của thỏa ước lao động tập thể như thế nào?\",\n",
    "    \"Hợp đồng lao động được giao kết theo hình thức nào?\",\n",
    "    \"Nội dung về đào tạo lao động có bắt buộc phải ghi vào hợp đồng lao động?\"\n",
    "]\n",
    "\n",
    "with open(\"search_results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    cnt = 1\n",
    "    for user_query in queries:\n",
    "        # Encode the question\n",
    "        query_vec = model.encode(user_query, normalize_embeddings=True)\n",
    "\n",
    "        # Query Qdrant to get results\n",
    "        hits = qdrant_client.query_points(\n",
    "            collection_name=\"laws_collection\",\n",
    "            query=query_vec.tolist(),\n",
    "            limit=20,\n",
    "            with_payload=True\n",
    "        ).points\n",
    "\n",
    "        seen_titles = set()\n",
    "        arti_cnt = 1\n",
    "\n",
    "        f.write(f\"Query {cnt}: {user_query}\\n\")\n",
    "\n",
    "        for hit in hits:\n",
    "            title = hit.payload['title'].strip()\n",
    "\n",
    "            if title in seen_titles:\n",
    "                continue\n",
    "\n",
    "            seen_titles.add(title)\n",
    "\n",
    "            f.write(f\"Article {arti_cnt}: {title}\\n\")\n",
    "            f.write(\"---\\n\")\n",
    "            arti_cnt += 1\n",
    "\n",
    "            # Stop if there are 5 unique results\n",
    "            if arti_cnt > 5:\n",
    "                break\n",
    "\n",
    "        f.write(\"\\n\\n\")\n",
    "        cnt += 1\n",
    "\n",
    "print(\"Results saved to file 'search_results.txt'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Get accuracy on a test set (10 pairs <question - correct rule>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "queries = []\n",
    "ground_truth_ids = []\n",
    "\n",
    "correct = 0\n",
    "\n",
    "for i, query in enumerate(queries):\n",
    "    query_vec = model.encode(query, normalize_embeddings=True)\n",
    "    hits = qdrant_client.query_points(\n",
    "        collection_name=\"laws_collection\",\n",
    "        query=query_vec.tolist(),\n",
    "        limit=5,   # Top K\n",
    "        with_payload=True\n",
    "    ).points\n",
    "\n",
    "    top_ids = [hit.payload['id'] for hit in hits]\n",
    "    if ground_truth_ids[i] in top_ids:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / len(queries)\n",
    "print(f\"Accuracy@5: {accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
